{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face and Facial Keypoint detection\n",
    "\n",
    "After you've trained a neural network to detect facial keypoints, you can then apply this network to *any* image that includes faces. The neural network expects a Tensor of a certain size as input and, so, to detect any face, you'll first have to do some pre-processing.\n",
    "\n",
    "1. Detect all the faces in an image using a face detector (we'll be using a Haar Cascade detector in this notebook).\n",
    "2. Pre-process those face images so that they are grayscale, and transformed to a Tensor of the input size that your net expects. This step will be similar to the `data_transform` you created and applied in Notebook 2, whose job was tp rescale, normalize, and turn any image into a Tensor to be accepted as input to your CNN.\n",
    "3. Use your trained model to detect facial keypoints on the image.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next python cell we load in required libraries for this section of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select an image \n",
    "\n",
    "Select an image to perform facial keypoint detection on; you can select any image of faces in the `images/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# load in color image for face detection\n",
    "image = cv2.imread('images/obamas.jpg')\n",
    "#image = cv2.imread('images/mona_lisa.jpg')\n",
    "#image = cv2.imread('images/the_beatles.jpg')\n",
    "\n",
    "\n",
    "# switch red and blue color channels \n",
    "# --> by default OpenCV assumes BLUE comes first, not RED as in many images\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# plot the image\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect all faces in an image\n",
    "\n",
    "Next, you'll use one of OpenCV's pre-trained Haar Cascade classifiers, all of which can be found in the `detector_architectures/` directory, to find any faces in your selected image.\n",
    "\n",
    "In the code below, we loop over each face in the original image and draw a red square on each face (in a copy of the original image, so as not to modify the original). You can even [add eye detections](https://docs.opencv.org/3.4.1/d7/d8b/tutorial_py_face_detection.html) as an *optional* exercise in using Haar detectors.\n",
    "\n",
    "An example of face detection on a variety of images is shown below.\n",
    "\n",
    "<img src='images/haar_cascade_ex.png' width=80% height=80%/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_haar(image):\n",
    "    # load in a haar cascade classifier for detecting frontal faces\n",
    "    face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # run the detector\n",
    "    # the output here is an array of detections; the corners of each detection box\n",
    "    # if necessary, modify these parameters until you successfully identify every face in a given image\n",
    "    faces = face_cascade.detectMultiScale(image, 1.2, 2)\n",
    "    return faces\n",
    "\n",
    "faces = run_haar(image)\n",
    "\n",
    "# make a copy of the original image to plot detections on\n",
    "image_with_detections = image.copy()\n",
    "\n",
    "# loop over the detected faces, mark the image where each face is found\n",
    "for (x,y,w,h) in faces:\n",
    "    # draw a rectangle around each detected face\n",
    "    # you may also need to change the width of the rectangle drawn depending on image resolution\n",
    "    cv2.rectangle(image_with_detections,(x,y),(x+w,y+h),(255,0,0),3) \n",
    "\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "\n",
    "plt.imshow(image_with_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in a trained model\n",
    "\n",
    "Once you have an image to work with (and, again, you can select any image of faces in the `images/` directory), the next step is to pre-process that image and feed it into your CNN facial keypoint detector.\n",
    "\n",
    "First, load your best model by its filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "#from models import Net\n",
    "\n",
    "#net = Net()\n",
    "\n",
    "## DONE: load the best saved model parameters (by your path name)\n",
    "## You'll need to un-comment the line below and add the correct name for *your* saved model\n",
    "# net.load_state_dict(torch.load('saved_models/keypoints_model_1.pt'))\n",
    "\n",
    "## print out your net and prepare it for testing (uncomment the line below)\n",
    "# net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose one of the following working models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "net, model_name = models.resnet18(weights=None), 'resnet18'\n",
    "net.fc=nn.Linear(net.fc.in_features, 16*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best performing model from (my) human perception.\n",
    "from fkpmodels.naimishnet import YaNaimishNet2\n",
    "net, model_name = YaNaimishNet2(), 'YaNaimishNet2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fkpmodels.naimishnet import YaNaimishNet3\n",
    "net, model_name = YaNaimishNet3(), 'YaNaimishNet3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'saved_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply load the model for testing after the newtwork architecture has been choosen above.\n",
    "checkpoint = torch.load(model_dir+model_name+'.pt')\n",
    "net.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_load import Rescale, Normalize, ToTensor, ToTensorRGB, FaceCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([Rescale(128),\n",
    "                                     ToTensorRGB()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoint detection\n",
    "\n",
    "Now, we'll loop over each detected face in an image (again!) only this time, you'll transform those faces in Tensors that your CNN can accept as input images.\n",
    "\n",
    "### DONE: Transform each detected face into an input Tensor\n",
    "\n",
    "You'll need to perform the following steps for each detected face:\n",
    "1. Convert the face from RGB to grayscale. --> Not done. Using RGB instead as described in notebook 2.\n",
    "2. Normalize the grayscale image so that its color range falls in [0,1] instead of [0,255]\n",
    "3. Rescale the detected face to be the expected square size for your CNN (224x224, suggested). --> Rescaling to 128x128.\n",
    "4. Reshape the numpy image into a torch image.\n",
    "\n",
    "You may find it useful to consult to transformation code in `data_load.py` to help you perform these processing steps.\n",
    "\n",
    "\n",
    "### DONE: Detect and display the predicted keypoints\n",
    "\n",
    "After each face has been appropriately converted into an input Tensor for your network to see as input, you'll wrap that Tensor in a Variable() and can apply your `net` to each face. The ouput should be the predicted the facial keypoints. These keypoints will need to be \"un-normalized\" for display, and you may find it helpful to write a helper function like `show_keypoints`. You should end up with an image like the following with facial keypoints that closely match the facial features on each individual face:\n",
    "\n",
    "<img src='images/michelle_detected.png' width=30% height=30%/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_load import norm_means, norm_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def plot_keypoints(key_pts):\n",
    "    def plot_keypoints_single(key_pts, offset=(0, 0), wh=128):\n",
    "        key_pts_copy = np.copy(key_pts)\n",
    "        # Invert keypoint normalization in data_load.py:Normalize: key_pts_copy = (key_pts_copy - 100)/50.0\n",
    "        key_pts_copy = key_pts_copy*50.0 + 100.0\n",
    "        key_pts_copy = key_pts_copy * wh / 128.0 + offset\n",
    "        plt.scatter(key_pts_copy[:, 0], key_pts_copy[:, 1], s=20, marker='.', c='m')\n",
    "    if isinstance(key_pts, list):\n",
    "        for face in key_pts:\n",
    "            plot_keypoints_single(face['pts'], offset=tuple(face['xywh'][0:2]), wh=face['xywh'][3])\n",
    "    else:\n",
    "        plot_keypoints_single(key_pts)\n",
    "\n",
    "\n",
    "def show_keypoints(image, key_pts, ax=None, normalize=True):\n",
    "    \"\"\"Show image with keypoints\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "        if normalize:\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            image = std * image + mean\n",
    "            image = np.clip(image, 0, 1)\n",
    "\n",
    "    #plt.imshow(image, cmap='gray')\n",
    "    plt.imshow(image)\n",
    "    plot_keypoints(key_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if it's available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#face = faces[435]\n",
    "#face = faces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_keypoints(image, faces, plot_faces=False):\n",
    "    image_copy = np.copy(image)\n",
    "\n",
    "    normalizer = transforms.Normalize(norm_means, norm_std)\n",
    "    resizer = transforms.Resize(128, antialias=True)\n",
    "\n",
    "    net.eval()\n",
    "    # loop over the detected faces from your haar cascade\n",
    "    #for (x,y,w,h) in np.array([face]):\n",
    "    output_pts_list = []\n",
    "    for (x,y,w,h) in faces:\n",
    "        # Select the region of interest that is the face in the image \n",
    "        roi = np.copy(image_copy[y:y+h, x:x+w])\n",
    "    \n",
    "        ## DONE: Reshape the numpy image shape (H x W x C) into a torch image shape (C x H x W)\n",
    "        roi = roi.transpose((2, 0, 1)).astype('float')/255.0\n",
    "        roi = np.asarray([roi])\n",
    "        roi_tt = torch.from_numpy(roi)\n",
    "        # Convert BGR image to RGB image\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        ## DONE: Normalize the grayscale/color image so that its color range falls in [0,1] instead of [0,255]\n",
    "        roi_tt = normalizer.forward(roi_tt)\n",
    "        ## DONE: Rescale the detected face to be the expected square size for your CNN (128x128)\n",
    "        roi_tt = resizer.forward(roi_tt)\n",
    "        # convert images to FloatTensors\n",
    "        roi_tt = roi_tt.type(torch.FloatTensor)\n",
    "        ## DONE: Make facial keypoint predictions using your loaded, trained network \n",
    "        ## perform a forward pass to get the predicted facial keypoints\n",
    "        roi_tt = roi_tt.to(device)\n",
    "        output_pts = net(roi_tt)\n",
    "        # reshape to batch_size x 16 x 2 pts\n",
    "        output_pts = output_pts.view(output_pts.size()[0], 16, -1)\n",
    "\n",
    "        output_pts_cpu = output_pts.cpu()[0].detach()\n",
    "        ## DONE: Display each detected face and the corresponding keypoints\n",
    "        if plot_faces:\n",
    "            show_keypoints(roi_tt.cpu()[0], output_pts_cpu)\n",
    "        output_pts_list.append({\n",
    "            'xywh': [x, y, w, h],\n",
    "            'pts': np.copy(output_pts_cpu),\n",
    "        })\n",
    "    return output_pts_list\n",
    "\n",
    "\n",
    "detect_keypoints(image, faces, plot_faces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE: Display orginal images with the keypoints of all faces\n",
    "def detect_and_show_keypoints(image_path, plot_faces=False):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    faces = run_haar(image)\n",
    "    output_pts_list = detect_keypoints(image, faces, plot_faces=plot_faces)\n",
    "    fig = plt.figure(figsize=(9,9))\n",
    "    show_keypoints(image, output_pts_list, ax=fig)\n",
    "\n",
    "\n",
    "detect_and_show_keypoints('images/obamas.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_and_show_keypoints('images/mona_lisa.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_and_show_keypoints('images/the_beatles.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://commons.wikimedia.org/wiki/File:Scientists_for_Future_2019-03-12_group_photograph_01.jpg\n",
    "detect_and_show_keypoints('images/Scientists_for_Future_2019-03-12_group_photograph_01.jpg', plot_faces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
